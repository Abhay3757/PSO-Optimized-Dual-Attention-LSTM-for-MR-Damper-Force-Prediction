# PSO-Optimized Dual-Attention LSTM for MR Damper Force Prediction

## ðŸš€ Project Overview

This project implements a sophisticated deep learning system for predicting MR (Magnetorheological) damper forces using a **Dual-Attention LSTM model** optimized with **Multi-Objective Particle Swarm Optimization (MOPSO)**. The system is designed for high accuracy, efficiency, and interpretability in time-series force prediction.

### ðŸŽ¯ Key Features

- **Dual-Attention LSTM Architecture**: Combined feature and temporal attention mechanisms
- **MOPSO Hyperparameter Optimization**: Multi-objective optimization balancing accuracy, efficiency, and generalization
- **CUDA-Accelerated Training**: GPU support for fast training and inference
- **Comprehensive Visualization**: Interactive plots and attention heatmaps for interpretability
- **Modular Design**: Clean, extensible codebase with separate modules for each component

## ðŸ“Š Dataset

- **Source**: Experimental MR damper data (`ranran_50_converted.csv`)
- **Features**: 4 input variables
  - **A**: Acceleration of Actuator
  - **D**: Damper Displacement  
  - **Y**: Voltage to MR Damper Circuit
  - **V**: Calculated velocity of MR damper
- **Target**: **F** - Damping Force
- **Size**: 74,068 samples with time-series structure

## ðŸ—ï¸ System Architecture

### 1. Data Preprocessing (`data_preprocessing.py`)
- Time-series window creation with configurable lookback
- Feature normalization (Standard/MinMax scaling)
- Time-aware train/validation/test splitting
- Missing value handling and data cleaning

### 2. Dual-Attention LSTM Model (`dual_attention_lstm.py`)
- **Feature Attention**: Learns importance weights for input variables at each timestep
- **Temporal Attention**: Learns importance weights for past timesteps
- **LSTM Backbone**: Sequential processing with configurable layers
- **Interpretable Outputs**: Attention weights for analysis

### 3. MOPSO Optimization (`mopso_optimizer.py`)
- **Hyperparameters Optimized**:
  - Number of LSTM layers (1-4)
  - Hidden units (32-256)
  - Learning rate (1e-5 to 1e-2)
  - Dropout rate (0.0-0.5)
  - Lookback window length (10-50)
- **Objectives**:
  - Minimize RMSE (accuracy) - 70% weight
  - Minimize inference latency (efficiency) - 20% weight
  - Minimize overfitting gap (generalization) - 10% weight

### 4. Training & Evaluation (`training.py`)
- Advanced training loop with early stopping
- Comprehensive metrics: RMSE, MAE, RÂ², MAPE
- Model checkpointing and best model saving
- Inference time measurement
- Attention mechanism analysis

### 5. Visualization (`visualization.py`)
- **Prediction Analysis**: Predicted vs actual force plots
- **Hysteresis Loops**: Force-displacement relationships
- **Force-Velocity Loops**: Damping characteristics
- **Attention Heatmaps**: Feature and temporal importance
- **Training Curves**: Loss convergence and learning rates
- **Performance Dashboard**: Comprehensive metrics display

### 6. Main Pipeline (`main_pipeline.py`)
- End-to-end workflow orchestration
- Configurable execution modes
- Results logging and saving
- Error handling and recovery

## ðŸ› ï¸ Installation & Setup

### Prerequisites
- Python 3.8+ (tested with 3.13.1)
- CUDA-capable GPU (recommended)
- Windows/Linux/macOS

### Environment Setup
```bash
# Clone the repository
cd "MR Damper/Model"

# Create virtual environment
python -m venv .venv

# Activate virtual environment
# Windows:
.venv\Scripts\activate
# Linux/macOS:
source .venv/bin/activate

# Install dependencies
pip install torch torchvision pandas numpy matplotlib seaborn scikit-learn pymoo tqdm plotly scipy psutil
```

### Verify Installation
```python
python -c "import torch; print(f'CUDA Available: {torch.cuda.is_available()}')"
```

## ðŸŽ® Usage

### Quick Start
```bash
# Run with default settings (quick test mode)
python main_pipeline.py --quick-test

# Create default configuration file
python main_pipeline.py --create-config

# Run with custom configuration
python main_pipeline.py --config pipeline_config.json
```

### Configuration Options

The system supports flexible configuration through JSON files:

```json
{
  "data": {
    "csv_path": "Dataset/ranran_50_converted.csv",
    "train_ratio": 0.7,
    "val_ratio": 0.15,
    "test_ratio": 0.15
  },
  "preprocessing": {
    "normalize_method": "standard",
    "lookback_window": 20
  },
  "optimization": {
    "enable_mopso": true,
    "mopso_config": {
      "population_size": 20,
      "max_generations": 10,
      "train_epochs": 50
    }
  },
  "training": {
    "batch_size": 64,
    "training_config": {
      "epochs": 100,
      "early_stopping_patience": 15
    }
  }
}
```

### Individual Module Testing
```bash
# Test data preprocessing
python data_preprocessing.py

# Test model architecture
python dual_attention_lstm.py

# Test training pipeline
python training.py

# Test MOPSO optimization
python mopso_optimizer.py

# Test visualizations
python visualization.py
```

## ðŸ“ˆ Performance Metrics

The system tracks comprehensive performance metrics:

- **Accuracy Metrics**:
  - RMSE (Root Mean Square Error)
  - MAE (Mean Absolute Error)
  - RÂ² (Coefficient of Determination)
  - MAPE (Mean Absolute Percentage Error)

- **Efficiency Metrics**:
  - Inference time (milliseconds)
  - Model size (MB)
  - Training time (seconds)
  - GPU memory usage

- **Interpretability Metrics**:
  - Feature importance rankings
  - Temporal attention patterns
  - Attention weight distributions

## ðŸ“ Output Structure

The pipeline generates organized results:

```
results/
â”œâ”€â”€ pipeline_results.json          # Complete results summary
â”œâ”€â”€ checkpoints/
â”‚   â”œâ”€â”€ best_model.pth             # Best trained model
â”‚   â””â”€â”€ checkpoint_epoch_*.pth     # Training checkpoints
â”œâ”€â”€ visualizations/
â”‚   â”œâ”€â”€ predictions_vs_actual.png  # Prediction accuracy
â”‚   â”œâ”€â”€ hysteresis_loops.png       # Force-displacement plots
â”‚   â”œâ”€â”€ force_velocity_loops.png   # Force-velocity relationships
â”‚   â”œâ”€â”€ attention_heatmaps.png     # Attention mechanisms
â”‚   â”œâ”€â”€ feature_importance.png     # Feature importance rankings
â”‚   â”œâ”€â”€ training_history.png       # Training progress
â”‚   â”œâ”€â”€ error_analysis.png         # Error distribution analysis
â”‚   â””â”€â”€ performance_metrics.png    # Performance dashboard
â””â”€â”€ mr_damper_pipeline.log         # Execution logs
```

## ðŸ”¬ Technical Details

### Model Architecture
- **Input Shape**: (batch_size, sequence_length, 4)
- **LSTM Layers**: 1-4 layers (optimized)
- **Hidden Units**: 32-256 (optimized)
- **Attention Mechanisms**: 
  - Feature Attention: Learns per-feature importance at each timestep
  - Temporal Attention: Learns per-timestep importance for prediction
- **Output**: Single force prediction value

### Optimization Strategy
- **Algorithm**: NSGA-II (Non-dominated Sorting Genetic Algorithm II)
- **Population Size**: 8-20 individuals
- **Generations**: 5-20 generations
- **Pareto Front**: Multiple optimal solutions balancing objectives
- **Selection**: Tournament selection with elitism

### Training Strategy
- **Loss Function**: Mean Squared Error (MSE)
- **Optimizer**: Adam with weight decay
- **Learning Rate**: Adaptive with plateau scheduling
- **Early Stopping**: Validation loss-based with patience
- **Gradient Clipping**: Prevents exploding gradients

## ðŸŽ¯ Results & Interpretability

### Attention Mechanism Insights
The dual-attention mechanism provides valuable insights:

1. **Feature Attention**: 
   - Identifies which input variables (A, D, Y, V) are most important
   - Shows temporal variation in feature importance
   - Helps understand damper physics

2. **Temporal Attention**:
   - Reveals which past timesteps influence current predictions
   - Shows memory length requirements
   - Identifies critical time windows

### Hysteresis Loop Analysis
- Captures nonlinear force-displacement relationships
- Visualizes damping characteristics
- Compares predicted vs actual damper behavior

### Performance Benchmarks
Typical performance on MR damper dataset:
- **RMSE**: ~15-30 N (depends on dataset scale)
- **RÂ²**: >0.90 (excellent correlation)
- **Inference Time**: ~1-3 ms per sample
- **Model Size**: <2 MB (efficient deployment)

## ðŸš€ Advanced Features

### Multi-Objective Optimization
The MOPSO approach balances three critical objectives:
1. **Accuracy** (RMSE minimization) - Primary objective
2. **Efficiency** (Inference time minimization) - Deployment consideration  
3. **Generalization** (Overfitting reduction) - Robustness requirement

### Real-time Capabilities
- Optimized for real-time inference (â‰¤5ms)
- GPU-accelerated predictions
- Batch processing support
- Memory-efficient implementation

### Extensibility
- Modular design for easy extension
- Support for additional input features
- Configurable attention mechanisms
- Plugin architecture for new optimizers

## ðŸ”§ Troubleshooting

### Common Issues

1. **CUDA not available**:
   ```bash
   # Install PyTorch with CUDA support
   pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118
   ```

2. **Memory issues**:
   - Reduce batch size in configuration
   - Decrease sequence length (lookback window)
   - Use gradient checkpointing

3. **Slow training**:
   - Enable CUDA acceleration
   - Increase batch size if memory allows
   - Reduce population size for MOPSO

4. **Poor convergence**:
   - Adjust learning rate range
   - Increase training epochs
   - Check data normalization

### Performance Optimization
- Use mixed precision training for speed
- Optimize batch size for your GPU memory
- Enable DataLoader num_workers for faster data loading
- Use torch.compile() for PyTorch 2.0+ acceleration

## ðŸ“ Citation

If you use this work in your research, please cite:

```bibtex
@software{mr_damper_dual_attention_lstm,
  title={PSO-Optimized Dual-Attention LSTM for MR Damper Force Prediction},
  author={[Abhay Singh]},
  year={2025},
  url={https://github.com/Abhay3757/PSO-Optimized-Dual-Attention-LSTM-for-MR-Damper-Force-Prediction}
}
```

## ðŸ“œ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ðŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request. For major changes, please open an issue first to discuss what you would like to change.

## ðŸ“ž Support

For questions, issues, or suggestions:
- Open an issue in the GitHub repository
- Check the troubleshooting section above
- Review the execution logs in `mr_damper_pipeline.log`

---

## ðŸ† Acknowledgments

- MR damper experimental data providers
- PyTorch and scikit-learn communities
- NSGA-II and MOPSO algorithm developers
- Open source visualization libraries (Matplotlib, Plotly, Seaborn)
